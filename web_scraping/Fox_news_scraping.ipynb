{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9905ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import boto3\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4464ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "raw_folder = \"../raw_scraped_data/\"\n",
    "analyzed_folder = \"../analyzed_data/\"\n",
    "raw_file = \"fox_news_economic_news.jsonl\"\n",
    "analyzed_file = \"fox_news_economic_sentiment.jsonl\"\n",
    "REGION = \"eu-west-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa691af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# 1. CONFIGURATION\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Base URL pattern where {page_num} will be replaced by the loop counter\n",
    "SEARCH_URL_PATTERN = \"https://www.foxnews.com/category/us/economy?page={}\"\n",
    "BASE_DOMAIN = \"https://www.foxnews.com\"\n",
    "\n",
    "# The loop will stop if it exceeds this maximum page number or finds an empty page.\n",
    "MAX_PAGES_TO_TRY = 60\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. EXTRACTION FUNCTION (Handles a single page)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def scrape_fox_news_page(page_num):\n",
    "    \"\"\"Fetches and parses articles from a single page number.\"\"\"\n",
    "    \n",
    "    current_url = SEARCH_URL_PATTERN.format(page_num)\n",
    "    print(f\"-> Loading Page {page_num}: {current_url}\")\n",
    "    \n",
    "    # Use a standard user-agent header\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(current_url, headers=headers, timeout=20)\n",
    "        response.raise_for_status() \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching page {page_num}: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Parse using lxml parser for speed\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    \n",
    "    # Find the main container holding all articles \n",
    "    article_list_container = soup.find('div', class_='content article-list')\n",
    "    \n",
    "    if not article_list_container:\n",
    "        # If the container is missing, the page structure might have changed, or the page is empty\n",
    "        print(f\"Warning: Article list container not found on Page {page_num}. Stopping.\")\n",
    "        return []\n",
    "\n",
    "    # Find all individual articles within that container\n",
    "    articles = article_list_container.find_all('article', class_='article')\n",
    "    \n",
    "    scraped_articles = []\n",
    "\n",
    "    for article in articles:\n",
    "        # --- 1. News Type (Eyebrow Category) ---\n",
    "        eyebrow_tag = article.find('span', class_='eyebrow')\n",
    "        news_type = eyebrow_tag.a.text.strip() if eyebrow_tag and eyebrow_tag.a else \"N/A\"\n",
    "\n",
    "        # Skip videos\n",
    "        if \"VIDEO\" in news_type.upper():\n",
    "            continue\n",
    "\n",
    "        # --- 2. Headline and Link ---\n",
    "        headline_tag = article.find('h4', class_='title')\n",
    "        link_tag = headline_tag.find('a', href=True) if headline_tag else None\n",
    "        \n",
    "        \n",
    "        if link_tag:\n",
    "            # Construct full URL for relative paths\n",
    "            url = urljoin(BASE_DOMAIN, link_tag.get('href'))\n",
    "            \n",
    "            scraped_articles.append({\n",
    "                'title': headline_tag.text.strip(),\n",
    "                'date': None,\n",
    "                'body': None,\n",
    "                'url': url\n",
    "            })\n",
    "    \n",
    "    return scraped_articles\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. MAIN EXECUTION: LOOP THROUGH ALL PAGES\n",
    "# ----------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    final_data = []\n",
    "    current_page = 1\n",
    "    \n",
    "    while current_page <= MAX_PAGES_TO_TRY:\n",
    "        # Scrape the data for the current page\n",
    "        page_data = scrape_fox_news_page(current_page)\n",
    "\n",
    "        # Add the collected articles to the final list\n",
    "        final_data.extend(page_data)\n",
    "        \n",
    "        print(f\"--- Finished processing Page {current_page}. Articles collected: {len(page_data)} ---\")\n",
    "        \n",
    "        # Increment to the next page\n",
    "        current_page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0729cae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_article(article_data):\n",
    "    \"\"\"\n",
    "    Takes the whole article dict, scrapes the URL, and returns\n",
    "    a tuple of (index, text, date) so we can map it back correctly.\n",
    "    \"\"\"\n",
    "    url = article_data[\"url\"]\n",
    "    idx = article_data[\"index\"] # We pass the index to keep track\n",
    "    \n",
    "    # Initialize defaults\n",
    "    article_text = None\n",
    "    article_date = None\n",
    "\n",
    "    try:\n",
    "        # standard timeout is good practice\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            return idx, None, None\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # ----- Extract article body -----\n",
    "        article_div = soup.find('div', class_='article-body')\n",
    "        if article_div:\n",
    "            paragraphs = article_div.find_all('p')\n",
    "            article_text = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "        # ----- Extract article date -----\n",
    "        date_span = soup.find('span', class_='article-date')\n",
    "        if date_span:\n",
    "            time_tag = date_span.find('time')\n",
    "            if time_tag and time_tag.has_attr('datetime'):\n",
    "                raw_date = time_tag['datetime']\n",
    "                article_date = raw_date.split(\"T\")[0]\n",
    "\n",
    "        return idx, article_text, article_date\n",
    "\n",
    "    except Exception as e:\n",
    "        return idx, None, None\n",
    "\n",
    "# --- 2. Setup Data for Processing ---\n",
    "# (Threads finish in random order, so we need the index to put data back in the right spot)\n",
    "indexed_data = []\n",
    "for i, article in enumerate(final_data):\n",
    "    # Create a lightweight object to pass to the worker\n",
    "    indexed_data.append({\"index\": i, \"url\": article[\"url\"]})\n",
    "\n",
    "# --- 3. Run in Parallel ---\n",
    "# max_workers=5 means 5 requests happen at the exact same time.\n",
    "\n",
    "print(f\"Scraping {len(indexed_data)} articles...\")\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    # Submit all tasks\n",
    "    futures = [executor.submit(scrape_article, item) for item in indexed_data]\n",
    "    \n",
    "    # Process results as they finish (tqdm shows progress bar)\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(indexed_data)):\n",
    "        idx, text, date = future.result()\n",
    "        \n",
    "        # Update the original list directly using the index we passed through\n",
    "        final_data[idx][\"body\"] = text\n",
    "        final_data[idx][\"date\"] = date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faece7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop any years before 2025\n",
    "final_data = [article for article in final_data if article[\"date\"] and article[\"date\"] >= \"2025-01-01\"]\n",
    "len(final_data)\n",
    "#save updated raw data\n",
    "with open(raw_folder + raw_file, 'w') as f:\n",
    "    for article in final_data:\n",
    "        f.write(json.dumps(article) + '\\n')\n",
    "print(f\"\\nUpdated {len(final_data)} articles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc1c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. Configuration & Client Initialization\n",
    "# ==========================================\n",
    "\n",
    "# Initialize Clients (Comprehend removed)\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name=REGION)\n",
    "\n",
    "# Use the EU Inference Profile ID for Nova Micro\n",
    "NOVA_MODEL_ID = \"eu.amazon.nova-micro-v1:0\"\n",
    "\n",
    "def is_related_to_economy(title):\n",
    "    \"\"\"\n",
    "    Uses Amazon Nova Micro to filter news titles.\n",
    "    Returns: True if related to US Economy, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Nova models accept a specific system prompt structure\n",
    "    system_prompt = (\n",
    "        \"You are a news classifier. \"\n",
    "        \"Your task is to analyze the user's news title and determine if it \"\n",
    "        \"loosely relates to one or more of the following: Economy, Stock Market, Financial Policy, Business, Prices, Inflation, Markets, Commodities, Incomes, Tax, Taxes, Crypto, GDP, Employment, Trade.\"\n",
    "        \"If you are not sure, default to YES.\"\n",
    "        \"Respond with ONLY one word: 'YES' or 'NO'.\"\n",
    "    )\n",
    "\n",
    "    # Nova request payload structure\n",
    "    payload = {\n",
    "        \"system\": [{\"text\": system_prompt}],\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [{\"text\": f\"Title: {title}\"}]\n",
    "            }\n",
    "        ],\n",
    "        \"inferenceConfig\": {\n",
    "            \"max_new_tokens\": 5,  # We only need a short YES/NO response\n",
    "            \"temperature\": 0.0    # Set to 0 for deterministic (consistent) results\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=NOVA_MODEL_ID,\n",
    "            body=json.dumps(payload)\n",
    "        )\n",
    "        \n",
    "        # Parse Nova response body\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        \n",
    "        # Navigate the Nova output structure\n",
    "        # output -> message -> content -> list -> text\n",
    "        model_answer = response_body[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        \n",
    "        # Clean and check answer\n",
    "        clean_answer = model_answer.strip().upper()\n",
    "        return \"YES\" in clean_answer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking title '{title}': {e}\")\n",
    "        return False\n",
    "\n",
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Uses Amazon Nova Micro to analyze sentiment regarding the US Administration.\n",
    "    Returns: \n",
    "        0 if Negative\n",
    "        1 if Positive)\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    # Truncate to a reasonable limit for the context window if needed\n",
    "    truncated_text = text[:10000]\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a political analyst. \"\n",
    "        \"Read the provided article text and answer the following question: \"\n",
    "        \"'Is the article more positive or negative considering the current/future state of the US economy? Be sure to consider both explicit statements and implicit tones.'\"\n",
    "        \"Respond with ONLY one of the following words: 'POSITIVE', 'NEGATIVE'. \"\n",
    "        \"Do not provide any explanation.\"\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        \"system\": [{\"text\": system_prompt}],\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [{\"text\": f\"Article Text: {truncated_text}\"}]\n",
    "            }\n",
    "        ],\n",
    "        \"inferenceConfig\": {\n",
    "            \"max_new_tokens\": 5,\n",
    "            \"temperature\": 0.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=NOVA_MODEL_ID,\n",
    "            body=json.dumps(payload)\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        model_answer = response_body[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        clean_answer = model_answer.strip().upper()\n",
    "        \n",
    "        # Map text answer to integer\n",
    "        if \"POSITIVE\" in clean_answer:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0 # Defaults to Negative for other outputs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Bedrock Sentiment Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==========================================\n",
    "# 2. Main Execution Flow\n",
    "# ==========================================\n",
    "\n",
    "# Example Data (Simulating your scraped table)\n",
    "articles_table = []\n",
    "# Ensure raw_folder and raw_file are defined in your environment\n",
    "try:\n",
    "    with open(raw_folder + raw_file, 'r') as f:\n",
    "        for line in f:\n",
    "            articles_table.append(json.loads(line))\n",
    "except NameError:\n",
    "    print(\"Error: raw_folder or raw_file not defined. Please define them before running.\")\n",
    "    articles_table = []\n",
    "\n",
    "print(f\"--- Processing {len(articles_table)} articles in {REGION} ---\\n\")\n",
    "\n",
    "processed_data = []\n",
    "\n",
    "for row in articles_table:\n",
    "    title = row.get('title')\n",
    "    body = row.get('body')\n",
    "    \n",
    "    print(f\"Checking: '{title}'\")\n",
    "    \n",
    "    # Step 1: Filter with Bedrock\n",
    "    if is_related_to_economy(body):\n",
    "        print(\"  [✓] Economy Related. Analyzing sentiment...\")\n",
    "        \n",
    "        # Step 2: Analyze Sentiment with Bedrock\n",
    "        sentiment_val = get_sentiment(body)\n",
    "        \n",
    "        if sentiment_val is not None:\n",
    "            row['is_economy'] = True\n",
    "            row['sentiment'] = sentiment_val\n",
    "            \n",
    "            processed_data.append(row)\n",
    "            print(f\"  -> Sentiment: {row['sentiment']} (0=NEGATIVE, 1=POSITIVE\")\n",
    "    else:\n",
    "        print(\"  [x] Unrelated. Skipping.\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Final Output\n",
    "print(\"\\n--- Final Results ---\")\n",
    "for item in processed_data:\n",
    "    print(f\"{item.get('date')} | {item.get('sentiment')} | {item.get('title')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa581797",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.DataFrame(articles_table)\n",
    "#keep only articels from 2025\n",
    "fox_news_economic_sentiment = articles_df.loc[(articles_df[\"is_economy\"] == True)]\n",
    "fox_news_economic_sentiment.to_json(analyzed_folder+analyzed_file, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d66bc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading data from: ../analyzed_data/fox_news_economic_sentiment.jsonl ---\n",
      "Processing 456 rows...\n",
      "\n",
      "--- Topic Totals (Articles containing topic) ---\n",
      "TOPIC                | TOTAL\n",
      "------------------------------\n",
      "topic_taxes          | 382  \n",
      "topic_jobs           | 259  \n",
      "topic_housing        | 251  \n",
      "topic_inflation      | 236  \n",
      "topic_energy         | 202  \n",
      "topic_stocks         | 70   \n",
      "topic_crypto         | 10   \n",
      "------------------------------\n",
      "\n",
      "Saving to ../analyzed_data/fox_news_economic_sentiment.jsonl...\n",
      "[✓] Done. File updated.\n"
     ]
    }
   ],
   "source": [
    "from helper_functions.topic_scraper import econ_topic_scaper\n",
    "econ_topic_scaper(analyzed_folder + analyzed_file,analyzed_folder, analyzed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba99729c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted batch of 1 files...\n",
      "Folder cleared.\n"
     ]
    }
   ],
   "source": [
    "from helper_functions.delete_folder_contents_from_s3 import delete_folder_contents_from_s3\n",
    "delete_folder_contents_from_s3(\"business-news-sentiments\", \"news_sentiments_monthly/combined_networks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0df16ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success! Uploaded ../analyzed_data/fox_news_economic_sentiment.jsonl to s3://business-news-sentiments/news_sentiments/fox/fox_news_economic_sentiment.jsonl\n"
     ]
    }
   ],
   "source": [
    "from helper_functions.upload_file_to_s3 import upload_file_to_s3\n",
    "upload_file_to_s3(analyzed_folder + analyzed_file,\"business-news-sentiments\", f\"news_sentiments/fox/{analyzed_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
