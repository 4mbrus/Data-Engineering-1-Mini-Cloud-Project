{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b03d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import concurrent.futures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a1d6454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths and other constants\n",
    "raw_folder = \"../raw_scraped_data/\"\n",
    "analyzed_folder = \"../analyzed_data/\"\n",
    "raw_file = \"cnn_economic_news.jsonl\"\n",
    "analyzed_file = \"cnn_economic_sentiment.jsonl\"\n",
    "REGION = \"eu-west-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0332a706",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words = [\"price\", \"prices\", \"inflation\", \"cost\", \"costs\", \"market\", \"markets\", \"stocks\", \"economy\", \"money\", \"tax\", \"taxes\", \"business\", \"commodities\", \"finances\", \"financial policy\", \"economic policy\", \"fiscal policy\", \"GDP\", \"unemployment\", \"interest rates\", \"recession\", \"economic growth\", \"budget deficit\", \"trade deficit\", \"consumer spending\", \"investment\", \"monetary policy\", \"fiscal stimulus\", \"housing market\", \"labor market\", \"wages\", \"corporate earnings\", \"supply chain\", \"energy prices\", \"commodity prices\", \"financial markets\", \"stock market volatility\", \"economic outlook\", \"economic indicators\", \"central bank policy\", \"inflation expectations\", \"currency exchange rates\", \"debt levels\", \"credit markets\", \"business cycles\", \"economic uncertainty\", \"global economy\", \"economic reforms\", \"tax policy changes\"]\n",
    "\n",
    "# --- Setup Firefox Options ---\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--width=1920\")\n",
    "options.add_argument(\"--height=1080\")\n",
    "options.page_load_strategy = 'eager' \n",
    "\n",
    "# --- Setup Service ---\n",
    "service = Service(log_output=\"geckodriver.log\") \n",
    "\n",
    "# Lists to store data\n",
    "unique_urls_list = [] # Stores just the unique links (strings)\n",
    "seen_links = set()  # Set for fast duplicate checking\n",
    "\n",
    "print(\"Attempting to launch Firefox...\")\n",
    "\n",
    "try:\n",
    "    driver = webdriver.Firefox(options=options, service=service)\n",
    "    driver.set_page_load_timeout(30) \n",
    "    print(\"Firefox launched successfully!\")\n",
    "\n",
    "    for key_word in key_words:\n",
    "        # Note: We are scraping page 1 (size 100) for each keyword\n",
    "        url = f\"https://edition.cnn.com/search?q={key_word}&from=0&size=100&page=1&sort=newest&types=article&section=\"\n",
    "        \n",
    "        print(f\"Scraping Keyword: {key_word}...\")\n",
    "        \n",
    "        try:\n",
    "            driver.get(url)\n",
    "        except Exception:\n",
    "            print(f\"Page load timeout for '{key_word}' (continuing anyway)...\")\n",
    "\n",
    "        try:\n",
    "            # Wait for headlines\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"container__headline-text\"))\n",
    "            )\n",
    "        except Exception:\n",
    "            print(f\"No results found for {key_word}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Parse content\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        headlines = soup.find_all(\"span\", class_=\"container__headline-text\")\n",
    "\n",
    "        new_count = 0\n",
    "        for h in headlines:\n",
    "            link = h.get(\"data-zjs-href\") \n",
    "            \n",
    "            # Fallback if link is not in the data attribute\n",
    "            if not link:\n",
    "                parent = h.find_parent(\"a\")\n",
    "                if parent:\n",
    "                    link = parent.get(\"href\")\n",
    "            \n",
    "            # --- UNIQUENESS CHECK ---\n",
    "            if link and link not in seen_links:\n",
    "                seen_links.add(link)\n",
    "                \n",
    "                # Add just the link to your simple list\n",
    "                unique_urls_list.append(link)\n",
    "                \n",
    "                new_count += 1\n",
    "        \n",
    "        print(f\"  Found {new_count} new unique links.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: {e}\")\n",
    "\n",
    "finally:\n",
    "    if 'driver' in locals():\n",
    "        driver.quit()\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Scraping complete.\")\n",
    "print(f\"Total unique links collected: {len(unique_urls_list)}\")\n",
    "\n",
    "# Printing the first 5 links as a preview\n",
    "print(\"Preview of unique links:\", unique_urls_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4aa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_urls_list\n",
    "#drop if link contains \"cnn-underscored\", which is a section about deals and product reviews\n",
    "unique_urls_list = [link for link in unique_urls_list if \"cnn-underscored\" not in link]\n",
    "#drop if link does not contain \"2025\"\n",
    "unique_urls_list = [link for link in unique_urls_list if \"/2025/\" in link]\n",
    "\n",
    "len(unique_urls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc6c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Setup Global Session ---\n",
    "# Using a session creates a connection pool, speeding up requests to the same domain.\n",
    "session = requests.Session()\n",
    "# Optional: Add a header to look like a real browser\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "})\n",
    "\n",
    "def process_url(url):\n",
    "    \"\"\"\n",
    "    Scrapes a single URL and returns the data dictionary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Date extraction (moved from the loop to here)\n",
    "        # Assuming the URL structure is consistent based on your slicing indices\n",
    "        year = url[20:24]\n",
    "        month = url[25:27]\n",
    "        day = url[28:30]\n",
    "        date = f\"{year}-{month}-{day}\"\n",
    "\n",
    "        # Request\n",
    "        response = session.get(url, timeout=10)\n",
    "        \n",
    "        # If the page errors out (e.g. 404), return partial data or skip\n",
    "        if response.status_code != 200:\n",
    "            return {\"title\": None, \"date\": date, \"body\": None, \"link\": url}\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "        # Headline\n",
    "        headline_tag = soup.find(\"h1\")\n",
    "        headline = headline_tag.get_text(strip=True) if headline_tag else None\n",
    "\n",
    "        # Article text\n",
    "        article_div = soup.find(\"div\", class_=\"article__content-container\")\n",
    "        if article_div:\n",
    "            paragraphs = article_div.find_all(\"p\")\n",
    "            article_text = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "        else:\n",
    "            article_text = None\n",
    "\n",
    "        return {\"title\": headline, \"date\": date, \"body\": article_text, \"link\": url}\n",
    "\n",
    "    except Exception as e:\n",
    "        # Return None or a log object so the main loop knows it failed\n",
    "        return None\n",
    "\n",
    "# --- 2. Run Parallel Scrape ---\n",
    "\n",
    "final_data = []\n",
    "\n",
    "# max_workers=10 is a safe starting point. \n",
    "print(f\"Scraping {len(unique_urls_list)} articles...\")\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Submit all tasks\n",
    "    futures = [executor.submit(process_url, url) for url in unique_urls_list]\n",
    "    \n",
    "    # Process as they complete\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(unique_urls_list)):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            final_data.append(result)\n",
    "\n",
    "# --- 3. Save to JSONL ---\n",
    "with open(raw_folder + raw_file, 'w') as f:\n",
    "    for article in final_data:\n",
    "        f.write(json.dumps(article) + '\\n')\n",
    "\n",
    "print(f\"All articles saved to {raw_folder + raw_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193c9bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. Configuration & Client Initialization\n",
    "# ==========================================\n",
    "\n",
    "# Initialize Clients (Comprehend removed)\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name=REGION)\n",
    "\n",
    "# Use the EU Inference Profile ID for Nova Micro\n",
    "NOVA_MODEL_ID = \"eu.amazon.nova-micro-v1:0\"\n",
    "\n",
    "def is_related_to_economy(title):\n",
    "    \"\"\"\n",
    "    Uses Amazon Nova Micro to filter news titles.\n",
    "    Returns: True if related to US Economy, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Nova models accept a specific system prompt structure\n",
    "    system_prompt = (\n",
    "        \"You are a news classifier. \"\n",
    "        \"Your task is to analyze the user's news title and determine if it \"\n",
    "        \"loosely relates to one or more of the following: Economy, Stock Market, Financial Policy, Business, Prices, Inflation, Markets, Commodities, Incomes, Tax, Crypto, GDP, Employment, Trade.\"\n",
    "        \"If you are not sure, default to YES.\"        \n",
    "        \"Respond with ONLY one word: 'YES' or 'NO'.\"\n",
    "    )\n",
    "\n",
    "    # Nova request payload structure\n",
    "    payload = {\n",
    "        \"system\": [{\"text\": system_prompt}],\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [{\"text\": f\"Title: {title}\"}]\n",
    "            }\n",
    "        ],\n",
    "        \"inferenceConfig\": {\n",
    "            \"max_new_tokens\": 5,  # We only need a short YES/NO response\n",
    "            \"temperature\": 0.0    # Set to 0 for deterministic (consistent) results\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=NOVA_MODEL_ID,\n",
    "            body=json.dumps(payload)\n",
    "        )\n",
    "        \n",
    "        # Parse Nova response body\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        \n",
    "        # Navigate the Nova output structure\n",
    "        # output -> message -> content -> list -> text\n",
    "        model_answer = response_body[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        \n",
    "        # Clean and check answer\n",
    "        clean_answer = model_answer.strip().upper()\n",
    "        return \"YES\" in clean_answer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking title '{title}': {e}\")\n",
    "        return False\n",
    "\n",
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Uses Amazon Nova Micro to analyze sentiment regarding the US Administration.\n",
    "    Returns: \n",
    "        0 if NO (Not positive)\n",
    "        1 if YES (Positive)\n",
    "        2 if MIXED\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    # Truncate to a reasonable limit for the context window if needed\n",
    "    truncated_text = text[:10000]\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a political analyst. \"\n",
    "        \"Read the provided article text and answer the following question: \"\n",
    "        \"'Is the article more positive or negative considering the current/future state of the US economy? Be sure to consider both explicit statements and implicit tones.'\"\n",
    "        \"Respond with ONLY one of the following words: 'POSITIVE', 'NEGATIVE'. \"\n",
    "        \"Do not provide any explanation.\"\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        \"system\": [{\"text\": system_prompt}],\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [{\"text\": f\"Article Text: {truncated_text}\"}]\n",
    "            }\n",
    "        ],\n",
    "        \"inferenceConfig\": {\n",
    "            \"max_new_tokens\": 5,\n",
    "            \"temperature\": 0.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=NOVA_MODEL_ID,\n",
    "            body=json.dumps(payload)\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        model_answer = response_body[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        clean_answer = model_answer.strip().upper()\n",
    "        \n",
    "        # Map text answer to integer\n",
    "        if \"POSITIVE\" in clean_answer:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0 # Defaults to Negative for \"NO\" or other outputs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Bedrock Sentiment Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==========================================\n",
    "# 2. Main Execution Flow\n",
    "# ==========================================\n",
    "\n",
    "# Example Data (Simulating your scraped table)\n",
    "articles_table = []\n",
    "# Ensure raw_folder and raw_file are defined in your environment\n",
    "try:\n",
    "    with open(raw_folder + raw_file, 'r') as f:\n",
    "        for line in f:\n",
    "            articles_table.append(json.loads(line))\n",
    "except NameError:\n",
    "    print(\"Error: raw_folder or raw_file not defined. Please define them before running.\")\n",
    "    articles_table = []\n",
    "\n",
    "print(f\"--- Processing {len(articles_table)} articles in {REGION} ---\\n\")\n",
    "\n",
    "processed_data = []\n",
    "\n",
    "for row in articles_table:\n",
    "    title = row.get('title')\n",
    "    body = row.get('body')\n",
    "    \n",
    "    print(f\"Checking: '{title}'\")\n",
    "    \n",
    "    # Step 1: Filter with Bedrock\n",
    "    if is_related_to_economy(body):\n",
    "        print(\"  [✓] Economy Related. Analyzing sentiment...\")\n",
    "        \n",
    "        # Step 2: Analyze Sentiment with Bedrock\n",
    "        sentiment_val = get_sentiment(body)\n",
    "        \n",
    "        if sentiment_val is not None:\n",
    "            row['is_economy'] = True\n",
    "            row['sentiment'] = sentiment_val\n",
    "            \n",
    "            processed_data.append(row)\n",
    "            print(f\"  -> Sentiment: {row['sentiment']} (0=NEGATIVE, 1=POSITIVE)\")\n",
    "    else:\n",
    "        print(\"  [x] Unrelated. Skipping.\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Final Output\n",
    "print(\"\\n--- Final Results ---\")\n",
    "for item in processed_data:\n",
    "    print(f\"{item.get('date')} | {item.get('sentiment')} | {item.get('title')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a541b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.DataFrame(articles_table)\n",
    "cnn_economic_sentiment = articles_df.loc[articles_df[\"is_economy\"] == True]\n",
    "cnn_economic_sentiment.to_json(analyzed_folder+analyzed_file, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1c6e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cnn_economic_sentiment))\n",
    "cnn_economic_sentiment[\"sentiment\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43892a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_WIKI_BUCKET = \"business-news-sentiments\"\n",
    "file_path_local = analyzed_folder + analyzed_file\n",
    "file_key_s3 = f\"news_sentiments/cnn/{analyzed_file}\"\n",
    "\n",
    "# Create S3 client\n",
    "s3 = boto3.client('s3', region_name= REGION)\n",
    "\n",
    "# Upload directly from disk\n",
    "try:\n",
    "    s3.upload_file(file_path_local, S3_WIKI_BUCKET, file_key_s3)\n",
    "    print(f\"✅ Success! Uploaded {file_path_local} to s3://{S3_WIKI_BUCKET}/{file_key_s3}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error uploading file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
